{
	"name": "WriteWeatherDataasDeltaTable",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "apachesparkpool",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d5c2e9ca-9c7f-4ee9-98d9-617652ee263b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/b8feecf6-2675-412f-9140-d5738847cc97/resourceGroups/BESS_Performance_Group/providers/Microsoft.Synapse/workspaces/synapsebess/bigDataPools/apachesparkpool",
				"name": "apachesparkpool",
				"type": "Spark",
				"endpoint": "https://synapsebess.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": false
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"# imports\n",
					"import requests\n",
					"import datetime\n",
					"import pandas as pd\n",
					"from py4j.java_gateway import java_import\n",
					"from pyspark.sql.functions import to_timestamp, min, max"
				],
				"execution_count": 96
			},
			{
				"cell_type": "code",
				"source": [
					"# Get coords of esVolta Anole project in DFW area\n",
					"lat, lon = 32.553638, -96.537577"
				],
				"execution_count": 97
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Determine Date Range of Interest\n",
					"#### (based off other dates from the other datasets)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# initialize variables\n",
					"base_path = \"abfss://datalake@bessstorage.dfs.core.windows.net/bronze/\"\n",
					"min_datetime = pd.NaT\n",
					"max_datetime = pd.NaT"
				],
				"execution_count": 98
			},
			{
				"cell_type": "code",
				"source": [
					"# determine delta table paths in Bronze directory\n",
					"java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
					"\n",
					"fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
					"status = fs.listStatus(spark._jvm.Path(base_path))\n",
					"delta_table_paths = [str(file.getPath()) for file in status if file.isDirectory()]"
				],
				"execution_count": 99
			},
			{
				"cell_type": "code",
				"source": [
					"for path in delta_table_paths:\n",
					"    df = spark.read.format(\"delta\").load(path)\n",
					"    table_name = path.split(\"/\")[-1]\n",
					"    df = df.withColumn(\"_time\", to_timestamp(\"_time\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
					"    df_min_datetime = df.select(min(\"_time\")).collect()[0][0]\n",
					"    df_max_datetime = df.select(max(\"_time\")).collect()[0][0]\n",
					"    print(f\"{table_name}: min datetime= {df_min_datetime}, max datetime= {df_max_datetime}\")\n",
					"\n",
					"    if pd.isna(min_datetime):\n",
					"        min_datetime = df_min_datetime\n",
					"    elif df_min_datetime < min_datetime:\n",
					"        min_datetime = df_min_datetime\n",
					"\n",
					"    if pd.isna(max_datetime):\n",
					"        max_datetime = df_max_datetime\n",
					"    elif df_max_datetime > max_datetime:\n",
					"        max_datetime = df_max_datetime\n",
					"\n",
					"print(f\"\\nweather min datetime: {min_datetime}, weather max datetime: {max_datetime}\")"
				],
				"execution_count": 101
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Implement NWS Weather API"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"point_url = f\"https://api.weather.gov/points/{lat},{lon}/stations\"\n",
					"point_resp = requests.get(point_url).json()"
				],
				"execution_count": 102
			},
			{
				"cell_type": "code",
				"source": [
					"station_name = point_resp['features'][0]['properties']['name']\n",
					"print('station name: ', station_name)"
				],
				"execution_count": 103
			},
			{
				"cell_type": "code",
				"source": [
					"station_id = point_resp['features'][0]['properties']['stationIdentifier']\n",
					"print('station id: ', station_id)"
				],
				"execution_count": 104
			},
			{
				"cell_type": "code",
				"source": [
					"observations_url = point_resp['observationStations'][0]\n",
					"print('observations url: ', observations_url)"
				],
				"execution_count": 105
			},
			{
				"cell_type": "code",
				"source": [
					"# initialize date range and request from observations endpoint\n",
					"start_str = min_datetime.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
					"end_str = max_datetime.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
					"\n",
					"observations_url = f\"https://api.weather.gov/stations/{station_id}/observations\"\n",
					"params = {\n",
					"    \"start\": start_str,\n",
					"    \"end\": end_str,\n",
					"    \"limit\": 100\n",
					"}\n",
					"\n",
					"observations_resp = requests.get(observations_url, params=params).json()\n",
					"\n",
					"# Display observations (temperature and timestamp)\n",
					"for obs in observations_resp.get(\"features\", []):\n",
					"    props = obs[\"properties\"]\n",
					"    temp = props[\"temperature\"][\"value\"]  # in Celsius\n",
					"    time = props[\"timestamp\"]\n",
					"    if temp is not None:\n",
					"        print(f\"{time}: {temp:.1f}Â°C\")"
				],
				"execution_count": 106
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Implement Open Meteo API\n",
					"#### (because NWS does not have beyond 7 days historical data)"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"min_datetime.strftime(\"%Y-%m-%d\")\n",
					"max_datetime.strftime(\"%Y-%m-%d\")"
				],
				"execution_count": 108
			},
			{
				"cell_type": "code",
				"source": [
					"url = \"https://api.open-meteo.com/v1/forecast\"\n",
					"params = {\n",
					"\t\"latitude\": lat,\n",
					"\t\"longitude\": lon,\n",
					"\t\"hourly\": \"temperature_2m\",\n",
					"\t\"start_date\": min_datetime.strftime(\"%Y-%m-%d\"),\n",
					"\t\"end_date\": max_datetime.strftime(\"%Y-%m-%d\")\n",
					"}\n",
					"response = requests.get(url, params=params)"
				],
				"execution_count": 111
			},
			{
				"cell_type": "code",
				"source": [
					"# Extract data from response\n",
					"hourly = response.json()['hourly']\n",
					"time = hourly['time']\n",
					"temperature = hourly['temperature_2m']\n",
					"\n",
					"# Convert into Spark df\n",
					"weather_data = [{'hour': t, 'temperature': temp} for t, temp in zip(time, temperature)]\n",
					"weather_df = spark.createDataFrame(weather_data)\n",
					"\n",
					"# Show df\n",
					"weather_df.limit(5).toPandas()"
				],
				"execution_count": 117
			},
			{
				"cell_type": "code",
				"source": [
					"# write to delta table\n",
					"weather_df.write.format(\"delta\").mode(\"overwrite\").save(base_path + 'weather_delta')"
				],
				"execution_count": 119
			}
		]
	}
}