{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Imports\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# Parameters cell\n",
        "csv_path = r'meter_meter_1_2025-04-30T20_53_43.157Z.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Initialize path variables\n",
        "base_path = r\"abfss://datalake@bessstorage.dfs.core.windows.net/bronze/\"\n",
        "csv_path = base_path + csv_path\n",
        "delta_path = csv_path.replace('.csv', '_delta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Read CSV into spark df\n",
        "df_spark = spark.read.option(\"header\", True).csv(csv_path)\n",
        "\n",
        "# Print top 5 rows of spark df\n",
        "df_spark.limit(5).toPandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Identify invalid column names\n",
        "invalid_columns = [col for col in df_spark.columns if re.search(r\"[^\\w]\", col)]\n",
        "print('invalid columns: ', str(invalid_columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Clean column names\n",
        "cleaned_columns = [re.sub(r\"[^\\w]\", \"\", col) for col in df_spark.columns]\n",
        "print('cleaned columns: ', str(cleaned_columns))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Write spark df as Delta Table\n",
        "df_cleaned = df_spark.toDF(*cleaned_columns)\n",
        "df_cleaned.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}