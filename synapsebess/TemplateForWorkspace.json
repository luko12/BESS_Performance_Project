{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synapsebess"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"synapsebess-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapsebess-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapsebess.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bessstorage.dfs.core.windows.net/"
		},
		"HttpServer1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/luko12/BESS_Performance_Project/main/"
		},
		"synapsebess-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bessstorage.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/ReadDataToBronze')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "ReadCSVsToBronze",
						"description": "Read CSVs from Github into Bronze and write as Delta Tables",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.filelist",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "CopyGithubCSVs",
									"description": "Copy CSVs from Github into Bronze",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "BinarySource",
											"storeSettings": {
												"type": "HttpReadSettings",
												"requestMethod": "GET"
											},
											"formatSettings": {
												"type": "BinaryReadSettings"
											}
										},
										"sink": {
											"type": "BinarySink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "GithubCSVBinary",
											"type": "DatasetReference",
											"parameters": {
												"relativePath": "@item()"
											}
										}
									],
									"outputs": [
										{
											"referenceName": "ADLSCSVBinary",
											"type": "DatasetReference",
											"parameters": {
												"outputFileName": "@last(split(item(), '/'))"
											}
										}
									]
								},
								{
									"name": "WriteCSVToDeltaTable",
									"description": "Write CSVs in Bronze as Delta Tables",
									"type": "SynapseNotebook",
									"dependsOn": [
										{
											"activity": "CopyGithubCSVs",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebook": {
											"referenceName": "WriteCSVasDeltaTable",
											"type": "NotebookReference"
										},
										"parameters": {
											"csv_path": {
												"value": {
													"value": "@last(split(item(), '/'))",
													"type": "Expression"
												},
												"type": "string"
											}
										},
										"snapshot": true,
										"sparkPool": {
											"referenceName": "apachesparkpool",
											"type": "BigDataPoolReference"
										},
										"executorSize": "Small",
										"conf": {
											"spark.dynamicAllocation.enabled": null,
											"spark.dynamicAllocation.minExecutors": null,
											"spark.dynamicAllocation.maxExecutors": null
										},
										"driverSize": "Small",
										"numExecutors": null
									}
								}
							]
						}
					},
					{
						"name": "WriteWeatherToDeltaTable",
						"description": "Write data from weather API into delta table",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "WriteWeatherDataasDeltaTable",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "apachesparkpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"filelist": {
						"type": "array",
						"defaultValue": [
							"Datasets/site__2025-04-30T20_50_35.909Z.csv",
							"Datasets/meter_meter_1_2025-04-30T20_53_43.157Z.csv",
							"Datasets/rtac_rtac_telemetry_2025-04-30T20_46_09.007Z.csv"
						]
					}
				},
				"annotations": [],
				"lastPublishTime": "2025-05-24T05:24:13Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/WriteWeatherDataasDeltaTable')]",
				"[concat(variables('workspaceId'), '/bigDataPools/apachesparkpool')]",
				"[concat(variables('workspaceId'), '/datasets/GithubCSVBinary')]",
				"[concat(variables('workspaceId'), '/datasets/ADLSCSVBinary')]",
				"[concat(variables('workspaceId'), '/notebooks/WriteCSVasDeltaTable')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ADLSCSVBinary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"outputFileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().outputFileName",
							"type": "Expression"
						},
						"folderPath": "bronze",
						"fileSystem": "datalake"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GithubCSVBinary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServer1",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"relativePath": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation",
						"relativeUrl": {
							"value": "@dataset().relativePath",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServer1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServer1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServer1_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsebess-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapsebess-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsebess-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapsebess-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WriteCSVasDeltaTable')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "apachesparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7a0126b5-eb18-472c-bbea-428010c5a41c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b8feecf6-2675-412f-9140-d5738847cc97/resourceGroups/BESS_Performance_Group/providers/Microsoft.Synapse/workspaces/synapsebess/bigDataPools/apachesparkpool",
						"name": "apachesparkpool",
						"type": "Spark",
						"endpoint": "https://synapsebess.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Imports\n",
							"import re"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# Parameters cell\n",
							"csv_path = r'meter_meter_1_2025-04-30T20_53_43.157Z.csv'"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"source": [
							"# Initialize path variables\n",
							"base_path = r\"abfss://datalake@bessstorage.dfs.core.windows.net/bronze/\"\n",
							"csv_path = base_path + csv_path\n",
							"delta_path = csv_path.replace('.csv', '_delta')"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"source": [
							"# Read CSV into spark df\n",
							"df_spark = spark.read.option(\"header\", True).csv(csv_path)\n",
							"\n",
							"# Print top 5 rows of spark df\n",
							"df_spark.limit(5).toPandas()"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"source": [
							"# Identify invalid column names\n",
							"invalid_columns = [col for col in df_spark.columns if re.search(r\"[^\\w]\", col)]\n",
							"print('invalid columns: ', str(invalid_columns))"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"source": [
							"# Clean column names\n",
							"cleaned_columns = [re.sub(r\"[^\\w]\", \"\", col) for col in df_spark.columns]\n",
							"print('cleaned columns: ', str(cleaned_columns))"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"source": [
							"# Write spark df as Delta Table\n",
							"df_cleaned = df_spark.toDF(*cleaned_columns)\n",
							"df_cleaned.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
						],
						"outputs": [],
						"execution_count": 48
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WriteWeatherDataasDeltaTable')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "apachesparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "d5c2e9ca-9c7f-4ee9-98d9-617652ee263b"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b8feecf6-2675-412f-9140-d5738847cc97/resourceGroups/BESS_Performance_Group/providers/Microsoft.Synapse/workspaces/synapsebess/bigDataPools/apachesparkpool",
						"name": "apachesparkpool",
						"type": "Spark",
						"endpoint": "https://synapsebess.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# imports\n",
							"import requests\n",
							"import datetime\n",
							"import pandas as pd\n",
							"from py4j.java_gateway import java_import\n",
							"from pyspark.sql.functions import to_timestamp, min, max"
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "code",
						"source": [
							"# Get coords of esVolta Anole project in DFW area\n",
							"lat, lon = 32.553638, -96.537577"
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Determine Date Range of Interest\n",
							"#### (based off other dates from the other datasets)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# initialize variables\n",
							"base_path = \"abfss://datalake@bessstorage.dfs.core.windows.net/bronze/\"\n",
							"min_datetime = pd.NaT\n",
							"max_datetime = pd.NaT"
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"source": [
							"# determine delta table paths in Bronze directory\n",
							"java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
							"\n",
							"fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
							"status = fs.listStatus(spark._jvm.Path(base_path))\n",
							"delta_table_paths = [str(file.getPath()) for file in status if file.isDirectory()]"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"source": [
							"for path in delta_table_paths:\n",
							"    df = spark.read.format(\"delta\").load(path)\n",
							"    table_name = path.split(\"/\")[-1]\n",
							"    df = df.withColumn(\"_time\", to_timestamp(\"_time\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
							"    df_min_datetime = df.select(min(\"_time\")).collect()[0][0]\n",
							"    df_max_datetime = df.select(max(\"_time\")).collect()[0][0]\n",
							"    print(f\"{table_name}: min datetime= {df_min_datetime}, max datetime= {df_max_datetime}\")\n",
							"\n",
							"    if pd.isna(min_datetime):\n",
							"        min_datetime = df_min_datetime\n",
							"    elif df_min_datetime < min_datetime:\n",
							"        min_datetime = df_min_datetime\n",
							"\n",
							"    if pd.isna(max_datetime):\n",
							"        max_datetime = df_max_datetime\n",
							"    elif df_max_datetime > max_datetime:\n",
							"        max_datetime = df_max_datetime\n",
							"\n",
							"print(f\"\\nweather min datetime: {min_datetime}, weather max datetime: {max_datetime}\")"
						],
						"outputs": [],
						"execution_count": 101
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Implement NWS Weather API"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"point_url = f\"https://api.weather.gov/points/{lat},{lon}/stations\"\n",
							"point_resp = requests.get(point_url).json()"
						],
						"outputs": [],
						"execution_count": 102
					},
					{
						"cell_type": "code",
						"source": [
							"station_name = point_resp['features'][0]['properties']['name']\n",
							"print('station name: ', station_name)"
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "code",
						"source": [
							"station_id = point_resp['features'][0]['properties']['stationIdentifier']\n",
							"print('station id: ', station_id)"
						],
						"outputs": [],
						"execution_count": 104
					},
					{
						"cell_type": "code",
						"source": [
							"observations_url = point_resp['observationStations'][0]\n",
							"print('observations url: ', observations_url)"
						],
						"outputs": [],
						"execution_count": 105
					},
					{
						"cell_type": "code",
						"source": [
							"# initialize date range and request from observations endpoint\n",
							"start_str = min_datetime.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
							"end_str = max_datetime.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
							"\n",
							"observations_url = f\"https://api.weather.gov/stations/{station_id}/observations\"\n",
							"params = {\n",
							"    \"start\": start_str,\n",
							"    \"end\": end_str,\n",
							"    \"limit\": 100\n",
							"}\n",
							"\n",
							"observations_resp = requests.get(observations_url, params=params).json()\n",
							"\n",
							"# Display observations (temperature and timestamp)\n",
							"for obs in observations_resp.get(\"features\", []):\n",
							"    props = obs[\"properties\"]\n",
							"    temp = props[\"temperature\"][\"value\"]  # in Celsius\n",
							"    time = props[\"timestamp\"]\n",
							"    if temp is not None:\n",
							"        print(f\"{time}: {temp:.1f}Â°C\")"
						],
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Implement Open Meteo API\n",
							"#### (because NWS does not have beyond 7 days historical data)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"min_datetime.strftime(\"%Y-%m-%d\")\n",
							"max_datetime.strftime(\"%Y-%m-%d\")"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"source": [
							"url = \"https://api.open-meteo.com/v1/forecast\"\n",
							"params = {\n",
							"\t\"latitude\": lat,\n",
							"\t\"longitude\": lon,\n",
							"\t\"hourly\": \"temperature_2m\",\n",
							"\t\"start_date\": min_datetime.strftime(\"%Y-%m-%d\"),\n",
							"\t\"end_date\": max_datetime.strftime(\"%Y-%m-%d\")\n",
							"}\n",
							"response = requests.get(url, params=params)"
						],
						"outputs": [],
						"execution_count": 111
					},
					{
						"cell_type": "code",
						"source": [
							"# Extract data from response\n",
							"hourly = response.json()['hourly']\n",
							"time = hourly['time']\n",
							"temperature = hourly['temperature_2m']\n",
							"\n",
							"# Convert into Spark df\n",
							"weather_data = [{'hour': t, 'temperature': temp} for t, temp in zip(time, temperature)]\n",
							"weather_df = spark.createDataFrame(weather_data)\n",
							"\n",
							"# Show df\n",
							"weather_df.limit(5).toPandas()"
						],
						"outputs": [],
						"execution_count": 117
					},
					{
						"cell_type": "code",
						"source": [
							"# write to delta table\n",
							"weather_df.write.format(\"delta\").mode(\"overwrite\").save(base_path + 'weather_delta')"
						],
						"outputs": [],
						"execution_count": 119
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/apachesparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 4,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "australiaeast"
		}
	]
}