{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "synapsebess"
		},
		"AzureDataLakeStorage1_accountKey": {
			"type": "secureString",
			"metadata": "Secure string for 'accountKey' of 'AzureDataLakeStorage1'"
		},
		"synapsebess-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'synapsebess-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:synapsebess.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"AzureDataLakeStorage1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bessstorage.dfs.core.windows.net/"
		},
		"HttpServer1_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://raw.githubusercontent.com/luko12/BESS_Performance_Project/main/"
		},
		"synapsebess-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://bessstorage.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/MergeDatasetsIntoSilverPipeline')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "MergeDatasetsIntoSilverBlock",
						"description": "Runs notebook to merge datasets together into cleaned dataset in silver",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "MergeDatasetsIntoSilverNotebook",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "apachesparkpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": []
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/MergeDatasetsIntoSilverNotebook')]",
				"[concat(variables('workspaceId'), '/bigDataPools/apachesparkpool')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ReadDataToBronze')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "ReadCSVsToBronze",
						"description": "Read CSVs from Github into Bronze and write as Delta Tables",
						"type": "ForEach",
						"dependsOn": [],
						"userProperties": [],
						"typeProperties": {
							"items": {
								"value": "@pipeline().parameters.filelist",
								"type": "Expression"
							},
							"activities": [
								{
									"name": "CopyGithubCSVs",
									"description": "Copy CSVs from Github into Bronze",
									"type": "Copy",
									"dependsOn": [],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"source": {
											"type": "BinarySource",
											"storeSettings": {
												"type": "HttpReadSettings",
												"requestMethod": "GET"
											},
											"formatSettings": {
												"type": "BinaryReadSettings"
											}
										},
										"sink": {
											"type": "BinarySink",
											"storeSettings": {
												"type": "AzureBlobFSWriteSettings"
											}
										},
										"enableStaging": false
									},
									"inputs": [
										{
											"referenceName": "GithubCSVBinary",
											"type": "DatasetReference",
											"parameters": {
												"relativePath": "@item()"
											}
										}
									],
									"outputs": [
										{
											"referenceName": "ADLSCSVBinary",
											"type": "DatasetReference",
											"parameters": {
												"outputFileName": "@last(split(item(), '/'))"
											}
										}
									]
								},
								{
									"name": "WriteCSVToDeltaTable",
									"description": "Write CSVs in Bronze as Delta Tables",
									"type": "SynapseNotebook",
									"dependsOn": [
										{
											"activity": "CopyGithubCSVs",
											"dependencyConditions": [
												"Succeeded"
											]
										}
									],
									"policy": {
										"timeout": "0.12:00:00",
										"retry": 0,
										"retryIntervalInSeconds": 30,
										"secureOutput": false,
										"secureInput": false
									},
									"userProperties": [],
									"typeProperties": {
										"notebook": {
											"referenceName": "WriteCSVasDeltaTable",
											"type": "NotebookReference"
										},
										"parameters": {
											"csv_path": {
												"value": {
													"value": "@last(split(item(), '/'))",
													"type": "Expression"
												},
												"type": "string"
											}
										},
										"snapshot": true,
										"sparkPool": {
											"referenceName": "apachesparkpool",
											"type": "BigDataPoolReference"
										},
										"executorSize": "Small",
										"conf": {
											"spark.dynamicAllocation.enabled": null,
											"spark.dynamicAllocation.minExecutors": null,
											"spark.dynamicAllocation.maxExecutors": null
										},
										"driverSize": "Small",
										"numExecutors": null
									}
								}
							]
						}
					},
					{
						"name": "WriteWeatherToDeltaTable",
						"description": "Write data from weather API into delta table",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "WriteWeatherDataasDeltaTable",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"sparkPool": {
								"referenceName": "apachesparkpool",
								"type": "BigDataPoolReference"
							},
							"executorSize": "Small",
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"driverSize": "Small",
							"numExecutors": null
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"parameters": {
					"filelist": {
						"type": "array",
						"defaultValue": [
							"Datasets/site__2025-04-30T20_50_35.909Z.csv",
							"Datasets/meter_meter_1_2025-04-30T20_53_43.157Z.csv",
							"Datasets/rtac_rtac_telemetry_2025-04-30T20_46_09.007Z.csv"
						]
					}
				},
				"annotations": [],
				"lastPublishTime": "2025-05-24T05:24:13Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/WriteWeatherDataasDeltaTable')]",
				"[concat(variables('workspaceId'), '/bigDataPools/apachesparkpool')]",
				"[concat(variables('workspaceId'), '/datasets/GithubCSVBinary')]",
				"[concat(variables('workspaceId'), '/datasets/ADLSCSVBinary')]",
				"[concat(variables('workspaceId'), '/notebooks/WriteCSVasDeltaTable')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ADLSCSVBinary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "AzureDataLakeStorage1",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"outputFileName": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "AzureBlobFSLocation",
						"fileName": {
							"value": "@dataset().outputFileName",
							"type": "Expression"
						},
						"folderPath": "bronze",
						"fileSystem": "datalake"
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/AzureDataLakeStorage1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/GithubCSVBinary')]",
			"type": "Microsoft.Synapse/workspaces/datasets",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"linkedServiceName": {
					"referenceName": "HttpServer1",
					"type": "LinkedServiceReference"
				},
				"parameters": {
					"relativePath": {
						"type": "string"
					}
				},
				"annotations": [],
				"type": "Binary",
				"typeProperties": {
					"location": {
						"type": "HttpServerLocation",
						"relativeUrl": {
							"value": "@dataset().relativePath",
							"type": "Expression"
						}
					}
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/linkedServices/HttpServer1')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AzureDataLakeStorage1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('AzureDataLakeStorage1_properties_typeProperties_url')]",
					"accountKey": {
						"type": "SecureString",
						"value": "[parameters('AzureDataLakeStorage1_accountKey')]"
					}
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/HttpServer1')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "HttpServer",
				"typeProperties": {
					"url": "[parameters('HttpServer1_properties_typeProperties_url')]",
					"enableServerCertificateValidation": true,
					"authenticationType": "Anonymous"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsebess-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('synapsebess-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/synapsebess-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('synapsebess-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create_bess_analytics_database')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE DATABASE bess_analytics;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "master",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/create_merged_silver_view')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE OR ALTER VIEW merged_delta_view AS\nSELECT * \nFROM OPENROWSET(\n    BULK 'https://bessstorage.dfs.core.windows.net/datalake/silver/merged_delta/',\n    FORMAT = 'DELTA'\n) AS rows;",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "bess_analytics",
						"poolName": "Built-in"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/MergeDatasetsIntoSilverNotebook')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "apachesparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "e337f07b-3a5f-44b9-83ab-5032c8d6726d"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b8feecf6-2675-412f-9140-d5738847cc97/resourceGroups/BESS_Performance_Group/providers/Microsoft.Synapse/workspaces/synapsebess/bigDataPools/apachesparkpool",
						"name": "apachesparkpool",
						"type": "Spark",
						"endpoint": "https://synapsebess.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Setup"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# imports\n",
							"import requests\n",
							"import datetime\n",
							"import pandas as pd\n",
							"from py4j.java_gateway import java_import\n",
							"from pyspark.sql.functions import to_timestamp, min, max, col, date_trunc, avg, last"
						],
						"outputs": [],
						"execution_count": 207
					},
					{
						"cell_type": "code",
						"source": [
							"base_path = r\"abfss://datalake@bessstorage.dfs.core.windows.net/\"\n",
							"bronze_path = base_path + r\"bronze/\"\n",
							"silver_path = base_path + r\"silver/\""
						],
						"outputs": [],
						"execution_count": 208
					},
					{
						"cell_type": "code",
						"source": [
							"# determine delta table paths in Bronze directory\n",
							"java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
							"\n",
							"fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
							"status = fs.listStatus(spark._jvm.Path(bronze_path))\n",
							"delta_table_paths = [str(file.getPath()) for file in status if file.isDirectory()]"
						],
						"outputs": [],
						"execution_count": 209
					},
					{
						"cell_type": "code",
						"source": [
							"# read delta tables as dataframes\n",
							"dfs = {}\n",
							"\n",
							"for path in delta_table_paths:\n",
							"    df = spark.read.format(\"delta\").load(path)\n",
							"    table_name = path.split(\"/\")[-1]\n",
							"\n",
							"    print('table name:', table_name)\n",
							"    dfs[table_name] = df"
						],
						"outputs": [],
						"execution_count": 210
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Print First 3 Rows of Tables"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfs['meter_meter_1_2025-04-30T20_53_43.157Z_delta'].limit(3).toPandas()"
						],
						"outputs": [],
						"execution_count": 211
					},
					{
						"cell_type": "code",
						"source": [
							"dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'].limit(3).toPandas()"
						],
						"outputs": [],
						"execution_count": 212
					},
					{
						"cell_type": "code",
						"source": [
							"dfs['site__2025-04-30T20_50_35.909Z_delta'].limit(3).toPandas()"
						],
						"outputs": [],
						"execution_count": 213
					},
					{
						"cell_type": "code",
						"source": [
							"dfs['weather_delta'].limit(3).toPandas()"
						],
						"outputs": [],
						"execution_count": 214
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Convert time columns to uniform datetime"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for df_key in dfs.keys():\n",
							"    if \"_time\" in dfs[df_key].columns:\n",
							"        time_col = \"_time\"\n",
							"        date_format = \"yyyy-MM-dd'T'HH:mm:ss'Z'\"\n",
							"    elif \"hour\" in dfs[df_key].columns:\n",
							"        time_col = \"hour\"\n",
							"        date_format = \"yyyy-MM-dd'T'HH:mm\"\n",
							"\n",
							"    dfs[df_key] = dfs[df_key].withColumn(\"datetime\", to_timestamp(time_col, date_format))\n",
							"    dfs[df_key] = dfs[df_key].drop(time_col)"
						],
						"outputs": [],
						"execution_count": 215
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Delete Null Rows"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for df_key in dfs.keys():\n",
							"    all_cols = dfs[df_key].columns\n",
							"\n",
							"    cols_to_check = [col for col in all_cols if col != \"datetime\"]\n",
							"\n",
							"    dfs[df_key] = dfs[df_key].na.drop(how=\"all\", subset=cols_to_check)"
						],
						"outputs": [],
						"execution_count": 216
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Change Column Datatypes Appropriately"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Meter table"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# print existing column types\n",
							"dfs['meter_meter_1_2025-04-30T20_53_43.157Z_delta'].printSchema()"
						],
						"outputs": [],
						"execution_count": 217
					},
					{
						"cell_type": "code",
						"source": [
							"# for each column, print top 3 most common values (to get a sense of appropriate type)\n",
							"for col_name in dfs['meter_meter_1_2025-04-30T20_53_43.157Z_delta'].columns:\n",
							"    if col_name not in [\"datetime\", \"_time\", \"hour\"]:\n",
							"        dfs['meter_meter_1_2025-04-30T20_53_43.157Z_delta'].groupBy(col_name) \\\n",
							"            .count() \\\n",
							"            .orderBy(\"count\", ascending=False) \\\n",
							"            .limit(3) \\\n",
							"            .show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 218
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Since float appears the most appropriate column type for each column, convert them to float"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# convert columns to float\n",
							"for col_name in dfs['meter_meter_1_2025-04-30T20_53_43.157Z_delta'].columns:\n",
							"    if col_name not in [\"datetime\", \"_time\", \"hour\"]:\n",
							"        dfs['meter_meter_1_2025-04-30T20_53_43.157Z_delta'] = dfs['meter_meter_1_2025-04-30T20_53_43.157Z_delta'].withColumn(col_name, col(col_name).cast(\"float\"))"
						],
						"outputs": [],
						"execution_count": 219
					},
					{
						"cell_type": "code",
						"source": [
							"# print datatypes again to confirm\n",
							"dfs['meter_meter_1_2025-04-30T20_53_43.157Z_delta'].printSchema()"
						],
						"outputs": [],
						"execution_count": 220
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### RTAC table"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# print existing column types\n",
							"dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'].printSchema()"
						],
						"outputs": [],
						"execution_count": 221
					},
					{
						"cell_type": "code",
						"source": [
							"# for each column, print top 3 most common values (to get a sense of appropriate type)\n",
							"for col_name in dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'].columns:\n",
							"    if col_name not in [\"datetime\", \"_time\", \"hour\"]:\n",
							"        dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'].groupBy(col_name) \\\n",
							"            .count() \\\n",
							"            .orderBy(\"count\", ascending=False) \\\n",
							"            .limit(3) \\\n",
							"            .show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 222
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"Here, it appears some columns would be best cast as boolean type however we should be okay converting them all to floats as before"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# convert columns to float\n",
							"for col_name in dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'].columns:\n",
							"    if col_name not in [\"datetime\", \"_time\", \"hour\"]:\n",
							"        dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'] = dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'].withColumn(col_name, col(col_name).cast(\"float\"))"
						],
						"outputs": [],
						"execution_count": 223
					},
					{
						"cell_type": "code",
						"source": [
							"# print datatypes again to confirm\n",
							"dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'].printSchema()"
						],
						"outputs": [],
						"execution_count": 224
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Site table"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# print existing column types\n",
							"dfs['site__2025-04-30T20_50_35.909Z_delta'].printSchema()"
						],
						"outputs": [],
						"execution_count": 225
					},
					{
						"cell_type": "code",
						"source": [
							"# for each column, print top 3 most common values (to get a sense of appropriate type)\n",
							"for col_name in dfs['site__2025-04-30T20_50_35.909Z_delta'].columns:\n",
							"    if col_name not in [\"datetime\", \"_time\", \"hour\"]:\n",
							"        dfs['site__2025-04-30T20_50_35.909Z_delta'].groupBy(col_name) \\\n",
							"            .count() \\\n",
							"            .orderBy(\"count\", ascending=False) \\\n",
							"            .limit(3) \\\n",
							"            .show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 226
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"In the site table, certain columns should definitely remain as strings whereas the rest should be cast to floats. In particular, the string columns are:\n",
							"* Application\n",
							"* ChaSt\n",
							"* LocRemCtl\n",
							"* Mode\n",
							"* Status"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# convert columns to float\n",
							"for col_name in dfs['site__2025-04-30T20_50_35.909Z_delta'].columns:\n",
							"    if col_name not in [\"datetime\", \"_time\", \"hour\", \"Application\", \"ChaSt\", \"LocRemCtl\", \"Mode\", \"Status\"]:\n",
							"        dfs['site__2025-04-30T20_50_35.909Z_delta'] = dfs['site__2025-04-30T20_50_35.909Z_delta'].withColumn(col_name, col(col_name).cast(\"float\"))"
						],
						"outputs": [],
						"execution_count": 227
					},
					{
						"cell_type": "code",
						"source": [
							"# print datatypes again to confirm\n",
							"dfs['site__2025-04-30T20_50_35.909Z_delta'].printSchema()"
						],
						"outputs": [],
						"execution_count": 228
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### Weather table"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfs['weather_delta'].dtypes"
						],
						"outputs": [],
						"execution_count": 229
					},
					{
						"cell_type": "code",
						"source": [
							"for col_name in dfs['weather_delta'].columns:\n",
							"    if col_name not in [\"datetime\", \"_time\", \"hour\"]:\n",
							"        dfs['weather_delta'].groupBy(col_name) \\\n",
							"            .count() \\\n",
							"            .orderBy(\"count\", ascending=False) \\\n",
							"            .limit(3) \\\n",
							"            .show(truncate=False)"
						],
						"outputs": [],
						"execution_count": 230
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"The weather data table is already properly types, so we don't need to convert any column types here"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Rename Ambiguous Columns"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'] = dfs['rtac_rtac_telemetry_2025-04-30T20_46_09.007Z_delta'].withColumnRenamed(\"lmp\", \"rtac_lmp\")\n",
							"dfs['site__2025-04-30T20_50_35.909Z_delta'] = dfs['site__2025-04-30T20_50_35.909Z_delta'].withColumnRenamed(\"lmp\", \"site_lmp\")"
						],
						"outputs": [],
						"execution_count": 231
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Group By Minute\n",
							"Take Avg of Float or Last of String Columns\n",
							"\n",
							"Skip weather table because it is already at hourly resolution"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"for df_key in dfs.keys():\n",
							"    if df_key != \"weather_delta\":\n",
							"        # order dataset\n",
							"        dfs[df_key] = dfs[df_key].orderBy(\"datetime\")\n",
							"\n",
							"        # calculate minute column rounded down\n",
							"        dfs[df_key] = dfs[df_key].withColumn(\"datetime_minute\", date_trunc(\"minute\", col(\"datetime\")))\n",
							"\n",
							"        # determine which columns are floats and which are strings\n",
							"        float_cols = [f.name for f in dfs[df_key].schema.fields if f.dataType.simpleString() == \"float\"]\n",
							"        string_cols = [f.name for f in dfs[df_key].schema.fields if f.dataType.simpleString() == \"string\"]\n",
							"\n",
							"        # build aggregation expression and perform the groupby\n",
							"        agg_exprs = [avg(col(c)).alias(c) for c in float_cols] + \\\n",
							"                [last(col(c), ignorenulls=True).alias(c) for c in string_cols]\n",
							"        dfs[df_key] = dfs[df_key].groupBy(\"datetime_minute\").agg(*agg_exprs)"
						],
						"outputs": [],
						"execution_count": 232
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Merge datasets together with weather dataset\n",
							"Based on LMP data showing maximum at midnight, minimum at noon, and local maximum at 2pm, the datasets appear to be in UTC. Since weather data is also in UTC, we don't need to perform any timezone conversion"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# merge site dataframes on datetime_minute column\n",
							"to_merge_tables = [table for table in list(dfs.keys()) if table != \"weather_delta\"]\n",
							"\n",
							"df_merged = dfs[to_merge_tables[0]].join(\n",
							"    dfs[to_merge_tables[1]], on=\"datetime_minute\", how=\"outer\")\n",
							"\n",
							"df_merged = df_merged.join(\n",
							"    dfs[to_merge_tables[2]], on=\"datetime_minute\", how=\"outer\")\n",
							"\n",
							"df_merged.limit(5).toPandas()"
						],
						"outputs": [],
						"execution_count": 233
					},
					{
						"cell_type": "code",
						"source": [
							"# calculate hour column rounded down in order to merge with hourly weather data\n",
							"df_merged = df_merged.withColumn(\"datetime_hour\", date_trunc(\"hour\", col(\"datetime_minute\")))\n",
							"df_merged.limit(5).toPandas()"
						],
						"outputs": [],
						"execution_count": 234
					},
					{
						"cell_type": "code",
						"source": [
							"# rename weather datetime column to \"datetime_hour\" because it is hourly\n",
							"dfs[\"weather_delta\"] = dfs[\"weather_delta\"].withColumnRenamed(\"datetime\", \"datetime_hour\")\n",
							"dfs[\"weather_delta\"].limit(5).toPandas()"
						],
						"outputs": [],
						"execution_count": 235
					},
					{
						"cell_type": "code",
						"source": [
							"# merge datasets with weather\n",
							"df_merged = df_merged.join(\n",
							"    dfs[\"weather_delta\"], on=\"datetime_hour\", how=\"outer\")\n",
							"df_merged.limit(5).toPandas()"
						],
						"outputs": [],
						"execution_count": 236
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Filter datetime_minute to where all datasets have data"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"latest_min_datetime = pd.NaT\n",
							"earliest_max_datetime = pd.NaT"
						],
						"outputs": [],
						"execution_count": 237
					},
					{
						"cell_type": "code",
						"source": [
							"for path in delta_table_paths:\n",
							"    if \"weather_delta\" not in path:\n",
							"        df = spark.read.format(\"delta\").load(path)\n",
							"        table_name = path.split(\"/\")[-1]\n",
							"        df = df.withColumn(\"_time\", to_timestamp(\"_time\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
							"        df_min_datetime = df.select(min(\"_time\")).collect()[0][0]\n",
							"        df_max_datetime = df.select(max(\"_time\")).collect()[0][0]\n",
							"        print(f\"{table_name}: min datetime= {df_min_datetime}, max datetime= {df_max_datetime}\")\n",
							"\n",
							"        if pd.isna(latest_min_datetime):\n",
							"            latest_min_datetime = df_min_datetime\n",
							"        elif df_min_datetime > latest_min_datetime:\n",
							"            latest_min_datetime = df_min_datetime\n",
							"\n",
							"        if pd.isna(earliest_max_datetime):\n",
							"            earliest_max_datetime = df_max_datetime\n",
							"        elif df_max_datetime < earliest_max_datetime:\n",
							"            earliest_max_datetime = df_max_datetime\n",
							"\n",
							"print(f\"\\nlatest min datetime: {latest_min_datetime}, earliest max datetime: {earliest_max_datetime}\")"
						],
						"outputs": [],
						"execution_count": 238
					},
					{
						"cell_type": "code",
						"source": [
							"df_merged = df_merged.filter(\n",
							"    (col(\"datetime_minute\") >= latest_min_datetime) & (col(\"datetime_minute\") <= earliest_max_datetime)\n",
							")\n",
							"df_merged.limit(5).toPandas()"
						],
						"outputs": [],
						"execution_count": 239
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Write into Delta Table in Silver Directory"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"df_merged.write.format(\"delta\").mode(\"overwrite\").save(silver_path + \"merged_delta\")"
						],
						"outputs": [],
						"execution_count": 240
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WriteCSVasDeltaTable')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "apachesparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "7a0126b5-eb18-472c-bbea-428010c5a41c"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b8feecf6-2675-412f-9140-d5738847cc97/resourceGroups/BESS_Performance_Group/providers/Microsoft.Synapse/workspaces/synapsebess/bigDataPools/apachesparkpool",
						"name": "apachesparkpool",
						"type": "Spark",
						"endpoint": "https://synapsebess.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# Imports\n",
							"import re"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"metadata": {
							"tags": [
								"parameters"
							]
						},
						"source": [
							"# Parameters cell\n",
							"csv_path = r'meter_meter_1_2025-04-30T20_53_43.157Z.csv'"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"source": [
							"# Initialize path variables\n",
							"base_path = r\"abfss://datalake@bessstorage.dfs.core.windows.net/bronze/\"\n",
							"csv_path = base_path + csv_path\n",
							"delta_path = csv_path.replace('.csv', '_delta')"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"source": [
							"# Read CSV into spark df\n",
							"df_spark = spark.read.option(\"header\", True).csv(csv_path)\n",
							"\n",
							"# Print top 5 rows of spark df\n",
							"df_spark.limit(5).toPandas()"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"source": [
							"# Identify invalid column names\n",
							"invalid_columns = [col for col in df_spark.columns if re.search(r\"[^\\w]\", col)]\n",
							"print('invalid columns: ', str(invalid_columns))"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"source": [
							"# Clean column names\n",
							"cleaned_columns = [re.sub(r\"[^\\w]\", \"\", col) for col in df_spark.columns]\n",
							"print('cleaned columns: ', str(cleaned_columns))"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"source": [
							"# Write spark df as Delta Table\n",
							"df_cleaned = df_spark.toDF(*cleaned_columns)\n",
							"df_cleaned.write.format(\"delta\").mode(\"overwrite\").save(delta_path)"
						],
						"outputs": [],
						"execution_count": 48
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WriteWeatherDataasDeltaTable')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "apachesparkpool",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "40aed98b-52ec-4651-9a50-43459cdd5a9f"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/b8feecf6-2675-412f-9140-d5738847cc97/resourceGroups/BESS_Performance_Group/providers/Microsoft.Synapse/workspaces/synapsebess/bigDataPools/apachesparkpool",
						"name": "apachesparkpool",
						"type": "Spark",
						"endpoint": "https://synapsebess.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/apachesparkpool",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.4",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"# imports\n",
							"import requests\n",
							"import datetime\n",
							"import pandas as pd\n",
							"from py4j.java_gateway import java_import\n",
							"from pyspark.sql.functions import to_timestamp, min, max"
						],
						"outputs": [],
						"execution_count": 96
					},
					{
						"cell_type": "code",
						"source": [
							"# Get coords of esVolta Anole project in DFW area\n",
							"lat, lon = 32.553638, -96.537577"
						],
						"outputs": [],
						"execution_count": 97
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Determine Date Range of Interest\n",
							"#### (based off other dates from the other datasets)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"# initialize variables\n",
							"base_path = \"abfss://datalake@bessstorage.dfs.core.windows.net/bronze/\"\n",
							"min_datetime = pd.NaT\n",
							"max_datetime = pd.NaT"
						],
						"outputs": [],
						"execution_count": 98
					},
					{
						"cell_type": "code",
						"source": [
							"# determine delta table paths in Bronze directory\n",
							"java_import(spark._jvm, 'org.apache.hadoop.fs.Path')\n",
							"\n",
							"fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
							"status = fs.listStatus(spark._jvm.Path(base_path))\n",
							"delta_table_paths = [str(file.getPath()) for file in status if file.isDirectory()]"
						],
						"outputs": [],
						"execution_count": 99
					},
					{
						"cell_type": "code",
						"source": [
							"for path in delta_table_paths:\n",
							"    df = spark.read.format(\"delta\").load(path)\n",
							"    table_name = path.split(\"/\")[-1]\n",
							"    df = df.withColumn(\"_time\", to_timestamp(\"_time\", \"yyyy-MM-dd'T'HH:mm:ss'Z'\"))\n",
							"    df_min_datetime = df.select(min(\"_time\")).collect()[0][0]\n",
							"    df_max_datetime = df.select(max(\"_time\")).collect()[0][0]\n",
							"    print(f\"{table_name}: min datetime= {df_min_datetime}, max datetime= {df_max_datetime}\")\n",
							"\n",
							"    if pd.isna(min_datetime):\n",
							"        min_datetime = df_min_datetime\n",
							"    elif df_min_datetime < min_datetime:\n",
							"        min_datetime = df_min_datetime\n",
							"\n",
							"    if pd.isna(max_datetime):\n",
							"        max_datetime = df_max_datetime\n",
							"    elif df_max_datetime > max_datetime:\n",
							"        max_datetime = df_max_datetime\n",
							"\n",
							"print(f\"\\nweather min datetime: {min_datetime}, weather max datetime: {max_datetime}\")"
						],
						"outputs": [],
						"execution_count": 101
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Implement NWS Weather API"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"point_url = f\"https://api.weather.gov/points/{lat},{lon}/stations\"\n",
							"point_resp = requests.get(point_url).json()"
						],
						"outputs": [],
						"execution_count": 102
					},
					{
						"cell_type": "code",
						"source": [
							"station_name = point_resp['features'][0]['properties']['name']\n",
							"print('station name: ', station_name)"
						],
						"outputs": [],
						"execution_count": 103
					},
					{
						"cell_type": "code",
						"source": [
							"station_id = point_resp['features'][0]['properties']['stationIdentifier']\n",
							"print('station id: ', station_id)"
						],
						"outputs": [],
						"execution_count": 104
					},
					{
						"cell_type": "code",
						"source": [
							"observations_url = point_resp['observationStations'][0]\n",
							"print('observations url: ', observations_url)"
						],
						"outputs": [],
						"execution_count": 105
					},
					{
						"cell_type": "code",
						"source": [
							"# initialize date range and request from observations endpoint\n",
							"start_str = min_datetime.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
							"end_str = max_datetime.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
							"\n",
							"observations_url = f\"https://api.weather.gov/stations/{station_id}/observations\"\n",
							"params = {\n",
							"    \"start\": start_str,\n",
							"    \"end\": end_str,\n",
							"    \"limit\": 100\n",
							"}\n",
							"\n",
							"observations_resp = requests.get(observations_url, params=params).json()\n",
							"\n",
							"# Display observations (temperature and timestamp)\n",
							"for obs in observations_resp.get(\"features\", []):\n",
							"    props = obs[\"properties\"]\n",
							"    temp = props[\"temperature\"][\"value\"]  # in Celsius\n",
							"    time = props[\"timestamp\"]\n",
							"    if temp is not None:\n",
							"        print(f\"{time}: {temp:.1f}°C\")"
						],
						"outputs": [],
						"execution_count": 106
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"# Implement Open Meteo API\n",
							"#### (because NWS does not have beyond 7 days historical data)"
						]
					},
					{
						"cell_type": "code",
						"source": [
							"min_datetime.strftime(\"%Y-%m-%d\")\n",
							"max_datetime.strftime(\"%Y-%m-%d\")"
						],
						"outputs": [],
						"execution_count": 108
					},
					{
						"cell_type": "code",
						"source": [
							"url = \"https://api.open-meteo.com/v1/forecast\"\n",
							"params = {\n",
							"\t\"latitude\": lat,\n",
							"\t\"longitude\": lon,\n",
							"\t\"hourly\": \"temperature_2m\",\n",
							"\t\"start_date\": min_datetime.strftime(\"%Y-%m-%d\"),\n",
							"\t\"end_date\": max_datetime.strftime(\"%Y-%m-%d\")\n",
							"}\n",
							"response = requests.get(url, params=params)"
						],
						"outputs": [],
						"execution_count": 111
					},
					{
						"cell_type": "code",
						"source": [
							"# Extract data from response\n",
							"hourly = response.json()['hourly']\n",
							"time = hourly['time']\n",
							"temperature = hourly['temperature_2m']\n",
							"\n",
							"# Convert into Spark df\n",
							"weather_data = [{'hour': t, 'temperature': temp} for t, temp in zip(time, temperature)]\n",
							"weather_df = spark.createDataFrame(weather_data)\n",
							"\n",
							"# Show df\n",
							"weather_df.limit(5).toPandas()"
						],
						"outputs": [],
						"execution_count": 117
					},
					{
						"cell_type": "code",
						"source": [
							"# write to delta table\n",
							"weather_df.write.format(\"delta\").mode(\"overwrite\").save(base_path + 'weather_delta')"
						],
						"outputs": [],
						"execution_count": 119
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/apachesparkpool')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 4,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.4",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "australiaeast"
		}
	]
}